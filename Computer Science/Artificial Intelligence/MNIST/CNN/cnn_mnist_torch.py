# -*- coding: utf-8 -*-
"""CNN_MNIST_Torch

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/cow-coding/School-Project/blob/master/Computer%20Science/Artificial%20Intelligence/MNIST/CNN/CNN_MNIST_Torch.ipynb

# Import Library
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init
from torch import optim
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt

device = 'cuda' if torch.cuda.is_available() else 'cpu'

if device == 'cuda':
  torch.cuda.manual_seed_all(777)

"""# valriable setting"""

batch_size_train = 64
batch_size_val = 64
batch_size_test = 1000

learning_rate = 0.01

"""# Data load"""

mnist_train = datasets.MNIST(root='MNIST_data/',
                             train=True,
                             transform=transforms.ToTensor(),
                             download=True)

mnist_test = datasets.MNIST(root='MNIST_data/',
                            train=False,
                            transform=transforms.ToTensor(),
                            download=True)

print('training data number : %d' %len(mnist_train))
print('test data number : %d' %len(mnist_test))

train_data, val_data = torch.utils.data.random_split(mnist_train, [50000, 10000])

train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size_train, shuffle=True)
test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size_test, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size_val, shuffle=True)

"""## Data Check"""

dataiter = iter(train_loader)
images, labels = dataiter.next()

print(images.shape)
print(labels.shape)

plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')

"""# Modeling"""

class CNN(nn.Module):

  def __init__(self):
    super(CNN, self).__init__()
    self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)
    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
    self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)
    self.fc1 = nn.Linear(4 * 4 * 128, 625, bias=True)
    self.fc2 = nn.Linear(625, 10)

  def forward(self, x):
    x = F.leaky_relu(self.conv1(x))
    x = F.max_pool2d(x, kernel_size=2, stride=2)
    x = F.leaky_relu(self.conv2(x))
    x = F.max_pool2d(x, kernel_size=2, stride=2)
    x = F.leaky_relu(self.conv3(x))
    x = F.max_pool2d(x, kernel_size=2, stride=2, padding=1)

    x = x.view(x.shape[0], -1)
    x = F.leaky_relu(self.fc1(x))
    x = self.fc2(x)

    return x

"""## Select Loss function"""

cnn = CNN().to(device)
criterion = nn.CrossEntropyLoss().to(device)

"""## Optimizer Setting"""

optimizer = optim.SGD(cnn.parameters(), lr=learning_rate)

"""## Training"""

epochs = 5

for epoch in range(epochs):
  
  for images, labels in train_loader:
    images = images.to(device)
    labels = labels.to(device)

    optimizer.zero_grad()

    output = cnn(images)
    loss = criterion(output, labels)

    loss.backward()
    optimizer.step()
  
  if epoch % 1 == 0:
    print("epoch {}, loss {}".format(epoch, loss.item()))

"""# Classification"""

corrects = 0

with torch.no_grad():
    for images, labels in val_loader:
        images = images.to(device)
        labels = labels.to(device)

        output = cnn(images)
        
        preds = output.argmax(dim=1, keepdim=True)
        corrects += preds.eq(labels.view_as(preds)).sum().item()

acc = corrects / 10000 * 100
print("Accuracy : {:.2f}%".format(acc))