# -*- coding: utf-8 -*-
"""MLP_MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JUA67sK6ELfgUFW22VDgKLswUd5PcbNz

# Import Library
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import optim
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt

torch.backends.cudnn.enabled = False
torch.manual_seed(1)

"""# valriable setting"""

batch_size_train = 64
batch_size_val = 64
batch_size_test = 1000

"""# Data load"""

standard = transforms.Compose([transforms.ToTensor(),
                               transforms.Normalize((0.5,), (0.5))])

train_data = datasets.MNIST(root='data/', train=True, transform=standard, download=True)
test_data = datasets.MNIST(root='data/', train=False, transform=standard, download=True)

print('training data number : %d' %len(train_data))
print('test data number : %d' %len(test_data))

train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000])

train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size_train, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size_val, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size_test, shuffle=True)

"""## Data Check"""

dataiter = iter(train_loader)
images, labels = dataiter.next()

print(images.shape)
print(labels.shape)

plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')

"""# Modeling"""

input_size = 784        # 28 x 28
output_size = 10        # 0 ~ 9

model = nn.Sequential(
    nn.Linear(input_size, 300),
    nn.LeakyReLU(),
    nn.Linear(300, 100),
    nn.LeakyReLU(),
    nn.Linear(100, 64),
    nn.LeakyReLU(),
    nn.Linear(64, output_size),
)

print(model)

"""## Select Loss function"""

criteria = nn.CrossEntropyLoss()

images, labels = next(iter(train_loader))
images = images.view(images.shape[0], -1)

loss = criteria(model(images), labels)

"""## Optimizer Setting"""

optimizer = optim.SGD(model.parameters(), lr=0.05)

"""## Training"""

epochs = 100

for epoch in range(epochs):
  
  for images, labels in train_loader:
    images = images.view(images.shape[0], -1)
    optimizer.zero_grad()

    output = model(images)
    loss = criteria(output, labels)

    loss.backward()
    optimizer.step()
  
  if epoch % 10 == 0:
    print("epoch {}, loss {}".format(epoch, loss.item()))

"""# Classification"""

corrects = 0

with torch.no_grad():
    for images, labels in val_loader:
        images = images.view(images.shape[0], -1)
        output = model(images)
        
        _, preds = torch.max(output, 1)
        corrects += torch.sum(preds == labels)

acc = corrects / 10000 * 100
print("Accuracy : %.4f" %acc)